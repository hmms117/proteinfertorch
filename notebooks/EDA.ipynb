{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torcheval.metrics.functional import binary_precision_recall_curve\n",
    "import torch\n",
    "from proteinfertorch.utils import read_pickle, read_fasta,save_to_fasta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, LinearLR, SequentialLR, LambdaLR\n",
    "\n",
    "#Simly DNN\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samirchar/miniconda3/envs/proteinfertorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/home/samirchar/miniconda3/envs/proteinfertorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.001\n",
    "lr_warmup_steps = 3_000\n",
    "lr_decay_steps = 1_000\n",
    "iters = 500_000\n",
    "lr_decay = 0.997\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "lr_warmup_scheule = LinearLR(optimizer=optim,\n",
    "                             start_factor=1/lr_warmup_steps,\n",
    "                             end_factor=1,\n",
    "                             total_iters=lr_warmup_steps\n",
    "                             )\n",
    "\n",
    "class ExponentialDecay:\n",
    "    def __init__(self, decay_steps, decay_rate, staircase):\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.staircase = staircase\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.staircase:\n",
    "            return self.decay_rate ** (step // self.decay_steps)\n",
    "        else:\n",
    "            return self.decay_rate ** (step / self.decay_steps)\n",
    "\n",
    "lr_decay_schedule = LambdaLR(optimizer = optim,\n",
    "                                  lr_lambda=ExponentialDecay(decay_steps=lr_decay_steps, decay_rate=lr_decay, staircase=True)\n",
    "                                  )\n",
    "\n",
    "lr_scheduler = SequentialLR(optimizer=optim,\n",
    "                            schedulers = [lr_warmup_scheule, lr_decay_schedule],\n",
    "                            milestones=[lr_warmup_steps])\n",
    "lrs = []\n",
    "for iteration in range(iters):\n",
    "    lr_scheduler.step()\n",
    "    lrs.append(optim.param_groups[0]['lr'])\n",
    "    # do train\n",
    "    # do val\n",
    "    if iteration == 4000:\n",
    "        break\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FASTA file to ../data/random_split/train_GO_1_batch.fasta\n",
      "Saved FASTA file to ../data/random_split/dev_GO_1_batch.fasta\n",
      "Saved FASTA file to ../data/random_split/test_GO_1_batch.fasta\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for split in [\"train\",\"dev\",\"test\"]:\n",
    "    data = read_fasta(f\"../data/random_split/{split}_GO.fasta\")\n",
    "    save_to_fasta(data[:40],f\"../data/random_split/{split}_GO_1_batch.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "model_ids = defaultdict(list)\n",
    "file_name2var_name = {\n",
    "    \"noxpd2_cnn_swissprot_go_clustered_swiss-cnn_for_swissprot_go_clustered\":\"GO_CLUSTERED_ENSEMBLE_ELEMENT_EXPERIMENT_IDS\",\n",
    "    \"noxpd2_cnn_swissprot_go_random_swiss-cnn_for_swissprot_go_random\":\"GO_RANDOM_ENSEMBLE_ELEMENT_EXPERIMENT_IDS\",\n",
    "    \"noxpd2_cnn_swissprot_ec_clustered_swiss-cnn_for_swissprot_ec_clustered\":\"EC_CLUSTERED_ENSEMBLE_ELEMENT_EXPERIMENT_IDS\",\n",
    "    \"noxpnd_cnn_swissprot_ec_random_swiss-cnn_for_swissprot_ec_random\":\"EC_RANDOM_ENSEMBLE_ELEMENT_EXPERIMENT_IDS\",\n",
    "\n",
    "}\n",
    "\n",
    "for i in pd.read_csv('../zipped_models.txt', header=None).values.flatten():\n",
    "    file_name = i.split('/')[-1].replace('.tar.gz', '')\n",
    "    #Only consider file_names with go or ec in them\n",
    "    if '-'.join(file_name.split('-')[:-1]) in file_name2var_name:\n",
    "        # Split string noxpd2_cnn_swissprot_ec_clustered_swiss-cnn_for_swissprot_ec_clustered-13704042.tar.gz' by - followed by number and extension\n",
    "        split = file_name.split('-')\n",
    "        w_id = int(split[-1])\n",
    "        name = '-'.join(split[:-1])\n",
    "        model_ids[file_name2var_name[name]].append(str(w_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_hdf(\"../outputs/test_labels_ProteInfer.h5\", key=\"labels_df\")\n",
    "probabilities = pd.read_hdf(\"../outputs/test_probabilities_ProteInfer.h5\", key=\"probabilities_df\")\n",
    "\n",
    "logits_binary = torch.tensor(probabilities.values.flatten(),device='cuda')\n",
    "labels_binary = torch.tensor(labels.values.flatten(),device='cuda')\n",
    "precision, recall, thresholds = binary_precision_recall_curve(logits_binary, labels_binary)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "best_th,best_f1 = thresholds[torch.argmax(f1)].item(),torch.max(f1).item()\n",
    "print(best_th,best_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proteinfertorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
